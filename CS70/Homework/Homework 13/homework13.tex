\documentclass{article}
%%%%%%% PREAMBLE %%%%%%%
%BEGIN_FOLD
%%%%% PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cabin} % section title font
\usepackage[default]{cantarell} % default font
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[scr]{rsfso} % power set symbol
\usepackage{tasks} % vaguely remember this being important for something...?
\usepackage{tikz} % diagrams
\usepackage{titlesec}
\usepackage{thmtools}
\usepackage{varwidth}
\usepackage{verbatim} % longer comments
\usepackage{xcolor}
%%%%%

%%%%% COLOURS
\definecolor{darkgreen}{HTML}{19A514}
\definecolor{lightgreen}{HTML}{9DFF9A}
\definecolor{darkblue}{HTML}{3E5FE4}
\definecolor{lightblue}{HTML}{BCDEFF}
\definecolor{darkred}{HTML}{CC3333}
\definecolor{lightred}{HTML}{FFA9A9}
\definecolor{darkpurple}{HTML}{A933CD}
\definecolor{lightpurple}{HTML}{F0BAFF}
\definecolor{darkyellow}{HTML}{D2D22A}
\definecolor{lightyellow}{HTML}{FFFFAE}
\definecolor{hyperlinkblue}{HTML}{3366CC}
%%%%%

%%%%% PAGE SETUP
% BASIC %
\setlength\parindent{0pt} % paragraph indentation
\setlength{\parskip}{5pt} % spacing between paragraphs
\usepackage[margin=1in]{geometry} % margin size

% HEADER/FOOTER %
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[R]{\thepage} % page number on bottom right
\fancyhead[R]{\textit{\leftmark}} % section title
\renewcommand{\headrulewidth}{0pt} % removing horizontal line at the top

% HYPERLINK FORMATTING %
\hypersetup{
	colorlinks,    
	linkcolor=hyperlinkblue,
	urlcolor=hyperlinkblue,
	pdftitle={...},
	pdfauthor={Michael Pham},
}

%%%%%

%%%%% ENVIRONMENTS STYLES
% SOLUTION ENVIRONMENT %
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

% PURPLE BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightpurple,
	linecolor=darkpurple,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkpurple}
]{purplebox}

% GREEN BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightgreen,
	linecolor=darkgreen,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkgreen}
]{greenbox}

% YELLOW BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightyellow,
	linecolor=darkyellow,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkyellow}
]{yellowbox}

% BLUE BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightblue,
	linecolor=darkblue,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkblue}
]{bluebox}

% RED BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightred,
	linecolor=darkred,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkred}
]{redbox}
%%%%%

%%%%% ENVIRONMENTS
% PURPLE BOXES (theorems, propositions, lemmas, and corollaries) %
\declaretheorem[style=purplebox,name=Theorem,within=section]{thm}
\declaretheorem[style=purplebox,name=Theorem,sibling=thm]{theorem}
\declaretheorem[style=purplebox,name=Theorem,numbered=no]{thm*, theorem*}
\declaretheorem[style=purplebox,name=Proposition,sibling=thm]{prop, proposition}
\declaretheorem[style=purplebox,name=Proposition,numbered=no]{prop*, proposition*}
\declaretheorem[style=purplebox,name=Lemma,sibling=thm]{lem, lemma}
\declaretheorem[style=purplebox,name=Lemma,numbered=no]{lem*, lemma*}
\declaretheorem[style=purplebox,name=Corollary,sibling=thm]{cor, corollary}
\declaretheorem[style=purplebox,name=Corollary,numbered=no]{cor*, corollary*}

% GREEN BOXES (definitions) %
\declaretheorem[style=greenbox,name=Definition,sibling=thm]{definition, defn}
\declaretheorem[style=greenbox,name=Definition,numbered=no]{definition*, defn*}

% BLUE BOXES (problems) %
\declaretheorem[style=bluebox,name=Problem,numberwithin=section]{homework, hw}
\declaretheorem[style=bluebox,name=Problem,numbered=no]{homework*, hw*}

% RED BOXES %
\declaretheorem[style=redbox,name=Remark,sibling=thm]{remark, rmk}
\declaretheorem[style=redbox,name=Remark, numbered=no]{remark*, rmk*}
\declaretheorem[style=yellowbox,name=Warning,sibling=thm]{warn}
\declaretheorem[style=yellowbox,name=Warning,numbered=no]{warn*}
%%%%%

%%%%% PROOF FORMATTING
\renewcommand\qedsymbol{$\blacksquare$}
%%%%%

%%% CUSTOM COMMANDS
% basic %
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\newsum}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\newprod}[2]{\prod\limits_{#1}^{#2}}
\newcommand{\newint}[2]{\int\limits_{#1}^{#2}}

% logic %
\newcommand*\xor{\oplus}
\newcommand{\all}{\forall}
\newcommand{\bland}{\bigwedge}
\newcommand{\blor}{\bigvee}
\newcommand*{\defeq}{\mathrel{\rlap{\raisebox{0.3ex}{$\m@th\cdot$}}\raisebox{-0.3ex}{$\m@th\cdot$}}=} \makeatother

% matrices %
\newcommand\aug{\fboxsep=- \fboxrule\!\!\!\fbox{\strut}\!\!\!}\makeatletter 

% sets %
\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

% probability stuff %
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\corr}{\mathrm{Corr}}

% title %
\newcommand{\mytitle}[2]{%
	\title{#1}
	\author{Michael Pham}
	\date{#2}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
}
%%%
%%%%%
%END_FOLD
%%%%%

\begin{document}
\mytitle{Homework 13}{Spring 2023}

\section{Just One Tail, Please}
Let $X$ be some random variable with finite mean and variance which is not necessarily non-negative.
The \textit{extended} version of Markov's Inequality states that for a non-negative function $\varphi(x)$ which is monotonically increasing for $x > 0$ and some constant $\alpha > 0$,
\[ \Pr\left[X \geq \alpha\right] \leq \frac{\E[\varphi(X)]}{\varphi(\alpha)} \]

Suppose $\E[X] = 0$, $\var(X) = \sigma^2 < \infty$, and $\alpha > 0$. 

\begin{hw}
	Use the extended version of Markov's Inequality stated above with $\varphi(x) = (x+c)^2$, where $c$ is some positive constant, to show that:
	\[ \Pr\left[X \geq \alpha\right] \le \frac{\sigma^2 + c^2}{(\alpha + c)^2} \]
\end{hw}
\begin{solution}
	We observe that for $\varphi = (x+c)^{2}$, we have the following:
	\begin{align*}
		\Pr[X \geq \alpha] &\leq \frac{\E[\varphi(X)]}{\varphi(\alpha)} \\
		&= \dfrac{\E\left[ (X + c)^{2} \right]}{(\alpha + c)^{2}} \\
		&= \dfrac{\E\left[ X^{2} + 2Xc + c^{2} \right]}{(\alpha+ c)^{2}} \\
		&= \dfrac{\E\left[ X^{2} \right] + \E\left[ 2Xc \right] + \E\left[ c^{2} \right]}{(\alpha + c)^{2}} \\
		&= \dfrac{\E[X^{2}] + 2c\E[X] + c^{2}\E[1]}{(\alpha + c)^{2}}
	\end{align*}

	From here, take note that $2c\E[X] = 0 = -\E[X]^{2}$. Therefore, we get:
	\begin{align*}
		\dfrac{\E[X^{2}] + 2c\E[X] + c^{2}\E[1]}{(\alpha + c)^{2}} &= \dfrac{\E[X^{2}] - \E[X]^{2} + c^{2}}{(\alpha + c)^{2}} \\
		&= \dfrac{\sigma^{2} + c^{2}}{(\alpha + c)^{2}} \\
		\therefore \Pr[X \geq \alpha] &\leq \dfrac{\sigma^{2} + c^{2}}{(\alpha + c)^{2}}
	\end{align*}
\end{solution}

\newpage

\begin{hw}
	Note that the above bound applies for all positive $c$, so we can choose a value of $c$ to minimize the expression, yielding the best possible bound. 
	Find the value for $c$ which will minimize the RHS expression (you may assume that the expression has a unique minimum).
\end{hw}
\begin{solution}
	In order to find the minimum for $\frac{\sigma^{2} + c^{2}}{(\alpha + c)^{2}}$, we can first differentiate it as such:
	\begin{align*}
		\dfrac{\mathrm{d}}{\mathrm{d}c} \dfrac{\sigma^{2} + c^{2}}{(\alpha + c)^{2}} &= \dfrac{\mathrm{d}}{\mathrm{d}c} \dfrac{\sigma^{2}}{(\alpha + c)^{2}} + \dfrac{\mathrm{d}}{\mathrm{d}c} \dfrac{c^{2}}{(\alpha + c)^{2}} \\
		&= \dfrac{\mathrm{d}}{\mathrm{d}c} \sigma^{2}(\alpha + c)^{-2} + \dfrac{\mathrm{d}}{\mathrm{d}c} c^{2}(\alpha + c)^{-2} \\
		&= -2\sigma^{2}(\alpha + c)^{-3} + 2c(\alpha + c)^{-2} - 2c^{2}(\alpha + c)^{-3} \\
		&= \dfrac{2c(\alpha + c) - 2\sigma^{2} - 2c^{2}}{(a+c)^{3}}
	\end{align*}

	Now, we will set this equal to zero to find the minimum:
	\begin{align*}
		\dfrac{2c(\alpha + c) - 2\sigma^{2} - 2c^{2}}{(a+c)^{3}} &= 0 \\
		2c(\alpha + c) - 2\sigma^{2} - 2c^{2} &= 0 \\
		c\alpha + c^{2} - \sigma^{2} - c^{2} &= 0 \\
		c\alpha - \sigma^{2} &= 0 \\
		c &= \dfrac{\sigma^{2}}{\alpha}
	\end{align*}

	Thus, we have that $c = \frac{\sigma^{2}}{\alpha}$.
\end{solution}

We can plug in the minimizing value of $c$ you found in part (b) to prove the following bound:
\[\Pr\left[X \geq \alpha\right] \leq \frac{\sigma^2}{\alpha^2 + \sigma^2}.\]
This bound is also known as Cantelli's inequality.

Now, recall that Chebyshev's inequality provides a two-sided bound.
That is, it provides a bound on \[\Pr\left[|X-\E[X]| \ge \alpha\right] = \Pr\left[X \ge \E[X] + \alpha\right] + \Pr\left[X \le \E[X] - \alpha\right]\]
If we only wanted to bound the probability of one of the tails, e.g. if we wanted to bound $\Pr\left[X \ge \E[X] + \alpha\right]$, it is tempting to just divide the bound we get from Chebyshev's by two.

\begin{hw}
	Why is this not always correct in general?
\end{hw}
\begin{solution}
	This is because we can't just assume that the distribution is symmetric; if a distribution isn't symmetric, then halving our bound from Chebyshev would result in an incorrect number.
\end{solution}

\newpage

\begin{hw}
	Provide an example of a random variable $X$ (does not have to be zero-mean) and a constant $\alpha$ such that using this method (dividing by two to bound one tail) is not correct,
	that is, $\Pr\left[X \ge \E[X] + \alpha\right] > \frac{\var(X)}{2\alpha^2}$
	or $\Pr\left[X \le \E[X] - \alpha\right] > \frac{\var(X)}{2\alpha^2}$.
\end{hw}
\begin{solution}
	Suppose we have a random variable $X$ which takes on values $\left\{  -1, 0, 1 \right\}$ with probability $\frac{1}{2}$ for $1$ and $\frac{1}{4}$ for $-1, 0$. We observe that $\E[X] = \frac{1}{4}$, and $\Var(X) = 1$.
	
	Now, we observe that for $\alpha = -\frac{3}{4}$, we get $\Pr[X \leq \E[X] - \alpha] = \Pr[X \leq 1] = 1$. Meanwhile, we see that $\frac{\Var(X)}{2\alpha^{2}} = \frac{1}{2\left( \frac{3}{4} \right)^{2}} = \frac{8}{9}$.
	
	We see then that $\Pr[X \leq \E[X] - \alpha] > \frac{\Var(X)}{2\alpha^{2}}$.
\end{solution}

Now we see the use of the bound proven in part (b) - it allows us to bound just one tail while still taking variance into account, and does not 
require us to assume any property of the random variable.
Note that the bound is also always guaranteed to be less than 1 (and therefore at least somewhat useful), unlike Markov's and Chebyshev's inequality!

Let's try out our new bound on a simple example. Suppose $X$ is a positively-valued random variable with $\E[X] = 3$ and $\var(X) = 2$. 

\begin{hw}
	What bound would Markov's inequality give for $\Pr[X \ge 5]$?
\end{hw}
\begin{solution}
	We observe that, using Markov's Inequality, we have:
	\begin{align*}
		\Pr[X \geq \alpha] &\leq \dfrac{\E[X]}{\alpha} \\
		\Pr[X \geq 5] &\leq \dfrac{3}{5}
	\end{align*}
\end{solution}

\begin{hw}
	What bound would Chebyshev's inequality give for $\Pr[X \ge 5]$?
\end{hw}
\begin{solution}
	We observe that, using Chebyshev's inequality, we get:
	\begin{align*}
		\Pr[X \geq 5] &= \Pr[\lvert X - \E[X] \rvert \geq 5 - \E[X]] \\
		&= \Pr[\lvert X - 3 \rvert \geq 2] \\
		&\leq \dfrac{1}{2}
	\end{align*}
\end{solution}

\begin{hw}
	What bound would Cantelli's Inequality give for $\Pr[X \ge 5]$? 
\end{hw}
\begin{solution}
	Since Cantelli's Inequality works for zero-mean only, we have to first shift it (but note that variance doesn't change when we shift). This yields us:
	\begin{align*}
		\Pr[X - 3 \geq 5 - 3] & \Pr[X - 3 \geq 2] \\
		&leq \dfrac{2}{4 + 2} \\
		&= \dfrac{2}{6} \\
		&= \dfrac{1}{3}
	\end{align*}
\end{solution}

\newpage

\section{Coupon Collector Variance}
It's that time of the year again---Safeway is offering its Monopoly Card promotion. Each time you visit Safeway, you are given one of $n$ different Monopoly Cards with equal probability. You need to collect them all to redeem the grand prize.

Let $X$ be the number of visits you have to make before you can redeem the grand prize. Remember that we've previously shown $\var(X) = n^2\left(\sum_{i=1}^n i^{-2}\right) - \E[X]$.

\begin{hw}
	The series $\sum_{i=1}^\infty i^{-2}$ converges to the constant value $\pi^2/6$. Using this fact and Chebyshev's Inequality, find a lower bound on $\beta$ for which the probability you need to make more than $\E[X] + \beta n$ visits is less than $1/100$, for large $n$.
\end{hw}
\begin{solution}
	To begin with, we will use Chebyshev's Inequality as follow:
	\begin{align*}
		\Pr[X \geq \E[X] + \beta n] &= \Pr[\lvert X - \E[X] \rvert \geq \beta n] \\
		&\leq \dfrac{\Var(X)}{(\beta n)^{2}} \\
		&= \dfrac{\Var(X)}{\beta^{2} n^{2}}
	\end{align*}

	We now want $\frac{\Var(X)}{\beta^{2} n^{2}}$ to be less than $\frac{1}{100}$ for large $n$:
	\begin{align*}
		\lim\limits_{n \rightarrow \infty} \dfrac{\Var(X)}{\beta^{2} n^{2}} &= \lim\limits_{n \rightarrow \infty} \dfrac{n^{2}\left( \sum_{i=1}^\infty i^{-2} \right) - \sum_{i = 1}^n i^{-1}}{\beta^{2} n^{2}} \\
		&= \lim\limits_{n \rightarrow \infty} \dfrac{\pi^{2}}{6\beta^{2}} - \lim\limits_{n \rightarrow \infty} \dfrac{\ln n}{n^{2}} \\
		&= \dfrac{\pi^{2}}{6\beta^{2}} - \lim\limits_{n \rightarrow \infty} \dfrac{\dfrac{1}{n}}{2n} \\
		&= \dfrac{\pi^{2}}{6\beta^{2}} - \lim\limits_{n \rightarrow \infty} \dfrac{1}{2n^{2}} \\
		&= \dfrac{\pi^{2}}{6\beta^{2}} \\
		\dfrac{\pi^{2}}{6\beta^{2}} &< \dfrac{1}{100} \\
		\beta &> \dfrac{10\pi}{\sqrt{6}}
	\end{align*}
\end{solution}

\newpage

\section{Estimating $\pi$}
In this problem, we discuss some interesting ways that you could probabilistically estimate $\pi$, and see how good these techniques are at estimating $\pi$.

\textbf{Technique 1:} Buffon's needle is a method that can be used to estimate the value of $\pi$. There is a table with infinitely many parallel lines spaced a distance 1 apart, and a needle of length 1. It turns out that if the needle is dropped uniformly at random onto the table, the probability of the needle intersecting a line is $\frac{2}{\pi}$. We have seen a proof of this in the notes.

\textbf{Technique 2:} Consider a square dartboard, and a circular target drawn inscribed in the square dartboard. A dart is thrown uniformly at random in the square. The probability the dart lies in the circle is $\frac{\pi}{4}$.

\textbf{Technique 3:} Pick two integers $x$ and $y$ independently and uniformly at random from $1$ to $M$, inclusive. Let $p_M$ be the probability that $x$ and $y$ are relatively prime. Then 
\[\lim_{M \to \infty} p_M = \frac{6}{\pi^2}.\]

Let $p_1 = \frac{2}{\pi}, p_2 = \frac{\pi}{4}$, and $p_3 = \frac{6}{\pi^2}$ be the probabilities of the desired events of \textbf{Technique 1}, \textbf{Technique 2}, and \textbf{Technique 3}, respectively. For each technique, we apply each technique $N$ times, then compute the proportion of the times each technique occurred, getting estimates $\hat{p_1}$, $\hat{p_2}$, and $\hat{p_3}$, respectively.

\begin{hw}
	For each $\hat{p_i}$, compute an expression $X_i$ in terms of $\hat{p_i}$ that would be an estimate of $\pi$.
\end{hw}
\begin{solution}
	We observe that for each $X_{i}$, we get the following:
	\begin{align*}
		X_{1} &= \dfrac{2}{\hat{p}_{1}} \\
		X_{2} &= \dfrac{4\pi^{2}}{\hat{p}_{2}} \\
		X_{3} &= \dfrac{6}{\pi \hat{p}_{3}}
	\end{align*}
\end{solution}

\begin{hw}
	Using Chebyshev's Inequality, compute the minimum value of $N$ such that $X_2$ is within $\varepsilon$ of $\pi$ with $1 - \delta$ confidence. Your answer should be in terms of $\varepsilon$ and $\delta$.
\end{hw}
\begin{solution}
	Using Chebyshev's Inequality, we get the following expression:
	\begin{align*}
		\Pr[\lvert X_{2} - \pi \rvert \geq \varepsilon] &\leq \dfrac{\Var(X_{2})}{(\varepsilon)^{2}} \\
		&= \dfrac{\dfrac{\dfrac{\pi}{4}\left( 1 - \dfrac{\pi}{4} \right)}{N}}{(\varepsilon)^{2}} \\
		&= \dfrac{3\pi^{2}}{16N(\varepsilon)^{2}} \\
		& \leq 1 - \delta \\
		N &\geq \dfrac{3\pi^{2}}{16(1-\delta)(\varepsilon)^{2}}
	\end{align*}
\end{solution}

\newpage

\section{Short Answer}
\begin{hw}
	Let $X$ be uniform on the interval $[0,2]$, and define $Y = 4X^2 + 1$. Find the PDF, CDF, expectation, and variance of $Y$.
\end{hw}
\begin{solution}
	For $Y = 4X^{2} + 1$, we get $\Pr[Y \leq y] = \Pr[4X^{2} + 1 \leq y] = \Pr\left[ X \leq \frac{\sqrt{y-1}}{2} \right]$. We also note that our bound is now $\left[ 1, 17 \right]$.
	
	Now, since we know that $X$ is uniform on the interval $\left[ 0, 2 \right]$, we get for the CDF:
	\begin{align*}
		\newint{0}{\frac{\sqrt{y-1}}{2}} \dfrac{1}{2} \mathrm{d}y = \dfrac{\sqrt{y-1}}{4}
	\end{align*}
	
	Now, to get the PDF, we simply differentiate the CDF to get:
	\[
		f(x) =
		\begin{cases*}
			\dfrac{1}{8\sqrt{y-1}} & $1 \leq y \leq 17$ \\
			0 & \text{elsewhere}
		\end{cases*}
	\]
	
	For the expected value, we see that
	\begin{align*}
		\E[Y] &= \newint{1}{17} yf(y) \mathrm{d}y \\
		&= \newint{1}{17} \dfrac{y}{8\sqrt{y-1}} \mathrm{d}y \\
		&= \dfrac{19}{3}
	\end{align*}

	For the variance, we observe the following:
	\begin{align*}
		\Var(Y) &= \E[Y^{2}] - \E[Y]^{2} \\
		&= \newint{1}{17} \dfrac{y^{2}}{8\sqrt{y-1}} \mathrm{d}y - \left( \newint{1}{17} \dfrac{y}{8\sqrt{y-1}} \mathrm{d}y \right)^{2} \\
			&= \dfrac{943}{15} - \dfrac{361}{9} \\
			&= \dfrac{1024}{45}
	\end{align*}
\end{solution}

\begin{hw}
	Let $X$ and $Y$ have joint distribution 
	\[
	f(x,y) = \begin{cases}
		c x y + \frac{1}{4} & \text{$x \in [1,2]$ and $y \in [0,2]$} \\
		0 & \text{otherwise.}
	\end{cases}
	\]
	Find the constant $c$. Are $X$ and $Y$ independent?
\end{hw}
\begin{solution}
	To find $c$, we have to make it so that the following holds:
	\begin{equation*}
		\int_{1}^{2} \int_{0}^{2} cxy + \dfrac{1}{4} \mathrm{d}y\mathrm{d}x = 1
	\end{equation*}

	After integrating the expression, we get:
	\begin{equation*}
		\dfrac{1}{4}cx^{2}y^{2} = 3c + \dfrac{1}{2}
	\end{equation*}

	Then, we see:
	\begin{equation*}
		c = \dfrac{1}{6}
	\end{equation*}

	Now, to see if they are independent or not, we first find the marginal distributions:
	\begin{align*}
		f_{x} &= \int_{0}^{2} \dfrac{xy}{6} + \dfrac{1}{4} \mathrm{d}y\\
		&= \dfrac{x}{3} + \dfrac{1}{2} \\
		f_{y} &= \int_{1}^{2} \dfrac{xy}{6} + \dfrac{1}{4} \mathrm{d}x\\
		&= \dfrac{y}{4} + \dfrac{1}{4}
	\end{align*}

	From here, we see that:
	\begin{align*}
		f_{x}f_{y} &= \left( \dfrac{x}{3} + \dfrac{1}{2} \right)\left( \dfrac{y}{4} + \dfrac{1}{4} \right) \\
		&= \dfrac{xy}{12} + \dfrac{x}{12} + \dfrac{y}{8} + \dfrac{1}{8} \\
		&= \dfrac{2xy + x + 3y + 3}{24} \\
		&\not= \dfrac{xy}{6} + \dfrac{1}{4}
	\end{align*}
	
	Therefore, we see that they're not independent.
\end{solution}

For the following two questions, let Let $X \sim \mathrm{Exp}(3)$
\begin{hw}
	Find probability that $X \in [0, \,1]$.
\end{hw}
\begin{solution}
	We want to find $\Pr[0 \leq X \leq 1]$, and since $X$ is an exponential function with mean $\mu = 3$, we get:
	\begin{align*}
		\int_{0}^{1} 3e^{-3x} \mathrm{d}x &= 1 - e^{-3}
	\end{align*}
\end{solution}
\begin{hw}
	Let $Y = \lfloor X \rfloor$. For each $k \in \NN$, what is the probability that $Y = k$? Write the distribution of $Y$ in terms of one of the famous distributions; provide that distribution's name and parameters.
\end{hw}
\begin{solution}
	We observe that for this question, we essentially want to find $\Pr[Y = k] = \Pr[\lfloor X \rfloor = k] = \Pr[k \leq X < k+1]$. Now, since $X$ is an exponential function, we get the following:
	\begin{align*}
		\Pr[k \leq X < k+1] &= \int_{k}^{k+1} 3e^{-3x} \mathrm{d}x \\
		&= e^{-3k}(1-e^{-3})
	\end{align*}

	We observe then that this is a geometric random variable, with $p = 1-e^{-3}$. Therefore, we have that $Y \sim \mathrm{Geometric}(1-e^{-3})$.
\end{solution}

\begin{hw}
	Let $X_i \sim \mathrm{Exp}(\lambda_i)$ for $i = 1,\ldots,n$ be mutually independent. It is a (very nice) fact that $\min(X_1,\ldots,X_n) \sim \mathrm{Exp}(\mu)$. Find $\mu$.
\end{hw}
\begin{solution}
	We observe that since $X_{i}$ is an exponential distribution, we get:
	\begin{equation*}
		\Pr[X_{i} \leq x] = 1 - e^{-\lambda_{i} x}
	\end{equation*}

	From here, we observe the following:
	\begin{align*}
		\Pr[Y \leq y] &= 1 - \Pr[Y > y] \\
		&= 1 - \Pr[\min(X_{1}, \ldots, X_{n}) > y] \\
		&= 1 - \Pr[X_{1} > y, \ldots, X_{n} > y] \\
		&= 1 - \Pr[X_{1} > y]\cdots\Pr[X_{n} > y] \\
		&= 1 - \left( e^{-\lambda_{1}y} \cdots e^{-\lambda_{n}y} \right) \\
		&= 1 - e^{-y\sum_{i = 1}^{n} \lambda_{i}}
	\end{align*}

	Therefore, we see that $\mu = \sum_{i=1}^{n} \lambda_i$.
\end{solution}

\newpage

\section{Useful Uniforms}
Let $X$ be a continuous random variable such that $\Pr[X \in (a,b)] > 0$ for all $a,b \in \mathbb{R}$ and $a < b$.

\begin{hw}
	Give an example of a distribution that $X$ could have, and one that it could not.
\end{hw}
\begin{solution}
	An example of a distribution which $X$ could have is a normal distribution $N(0,1)$, while a distribution which it can't have would be a uniform distribution $U(0,1)$.
\end{solution}

\begin{hw}
	Show that the CDF $F$ of $X$ is strictly increasing. That is, $F(x+\varepsilon) > F(x)$ for any $\varepsilon > 0$. Argue why this implies that $F: \mathbb{R} \to (0,1)$ must be invertible.
\end{hw}
\begin{solution}
	Recall that the slope of the CDF is the PDF. Now, we observe that because $\Pr[X \in (a,b)]$ is always positive, meaning that the PDF is always positive, we observe then the slope of the CDF must always be positive as well; in other words, it's strictly increasing. 
	
	Now, because the CDF is strictly increasing, it means then that it passes the horizontal line test; in other words, a horizontal line intersects the CDF in all places at exactly one point. As such, we see then that the CDF $F$ must be invertible.
\end{solution}

\begin{hw}
	Let $U$ be a uniform random variable on $(0,1)$. What is the
	distribution of $F^{-1}(U)$?
\end{hw}
\begin{solution}
	Keeping in mind the fact that $U$ is a random variable, we observe the following:
	\begin{align*}
		\Pr[F^{-1}(U) \leq x] &= \Pr[U \leq F(x)] \\
		&= \dfrac{F(x) - 0}{1 - 0} \\
		&= F(x)
	\end{align*}
\end{solution}

\begin{hw}
	Your work in part (c) shows that in order to sample $X$, it is enough
	to be able to sample $U$. If $X$ was a discrete random variable instead,
	taking finitely many values, can we still use $U$ to sample $X$?
\end{hw}
\begin{solution}
	No; this is because we're going from an uncountable set $U$ to a countable set $X$.
\end{solution}

\newpage

\section{Waiting For the Bus}
Edward and Jerry are waiting at the bus stop outside of Soda Hall.

Like many bus systems, buses arrive in periodic intervals. However, the Berkeley bus system is unreliable, so the length of these intervals are random, and follow Exponential distributions.

Edward is waiting for the 51B, which arrives according to an Exponential distribution with parameter $\lambda$. That is, if we let the random variable $X_i$ correspond to the difference between the arrival time $i$th and $(i-1)$st bus (also known as the inter-arrival time) of the 51B, $X_i \sim \mathrm{Exp}(\lambda)$.

Jerry is waiting for the 79, whose inter-arrival times also follows Exponential distributions with parameter $\mu$. That is, if we let $Y_i$ denote the inter-arrival time of the 79, $Y_i \sim \mathrm{Exp}(\mu)$. Assume that all inter-arrival times are independent.

\begin{hw}
	What is the probability that Jerry's bus arrives before Edward's bus?
\end{hw}
\begin{solution}
	We want to find $\Pr[Y_{i} < X_{i}]$. Keeping in mind the fact that $X_{i}$ and $Y_{i}$ are exponential distributions which are independent of each other, we see that this is the same as asking the following:
	\begin{align*}
		\int_{0}^{\infty} \Pr[X_{i} = k \cap Y_{i} < k] \mathrm{d}k &= \int_{0}^{\infty} \Pr[X_{i} = k] \Pr[ Y_{i} < k] \mathrm{d}k \\
		&= \int_{0}^{\infty} \lambda e^{-\lambda k}\left( 1 - e^{-\mu k} \right) \mathrm{d}k \\
		&= \int_{0}^{\infty} \lambda e^{-\lambda k} \mathrm{d}k - \int_{0}^{\infty} e^{-\mu k} \mathrm{d}k \\
		&= 1 - \dfrac{\lambda}{\lambda + \mu} \\
		&= \dfrac{\mu}{\lambda + \mu}
	\end{align*}
\end{solution}

\begin{hw}
	After 20 minutes, the 79 arrives, and Jerry rides the bus. However, the 51B still hasn't arrived yet. Let $D$ be the additional amount of time Edward needs to wait  for the 51B to arrive. What is the distribution of $D$?
\end{hw}
\begin{solution}
	We observe that, by the exponential distribution's memoryless property, we get that $D \sim \mathrm{Exp}(\mu)$.
\end{solution}

\begin{hw}
	Lavanya isn't picky, so she will wait until either the 51B or the 79 bus arrives. Find the distribution of $Z$, the amount of time Lavanya will wait before catching her bus.
\end{hw}
\begin{solution}
	Let $Z = \min(X_{i}, Y_{i})$. We observe then that:
	\begin{align*}
		\Pr[Z > z] &= \Pr[\min(X_{i}, Y_{i}) > z] \\
		&= \Pr[X_{i} > z, Y_{i} > z] \\
		&= \Pr[X_{i} > z]\Pr[Y_{i} > z] \\
		&= \left( e^{-\mu t} \right)\left( e^{-\lambda t} \right) \\
		&= e^{-\left( \mu + \lambda \right) t}
	\end{align*}

	Then, from here, we see that the CDF $\Pr[Z \leq z] = 1 - e^{-\left( \mu + \lambda \right)t}$; therefore, we observe that $Z ~ \mathrm{Exp}\left( \mu + \lambda \right)$.
\end{solution}

\begin{hw}
	Khalil doesn't feel like riding the bus with Edward. He decides that he will wait for the second arrival of the 51B to ride the bus. Find the distribution of $T = X_1 + X_2$, the amount of time that Khalil will wait to ride the bus.
\end{hw}
\begin{solution}
	To begin with, we want to find the CDF of $T$; that is, we want to find $\Pr[T \leq x]$.
	
	To do this, we do the following:
	\begin{align*}
		\Pr[T \leq x] &= \Pr[X_{1} + X_{2} \leq x] \\
		&= \int_{0}^{x} \int_{0}^{x-x_{2}} \Pr[X_{1} = x_{1}]\Pr[X_{2} = x_{2}] \mathrm{d}x_1\mathrm{d}x_2 \\
		&= \int_{0}^{x} \int_{0}^{x-x_{2}} \lambda e^{-\lambda x_{1}} \lambda e^{-\lambda x_{2}} \mathrm{d}x_1\mathrm{d}x_2 \\
		&= 1 - e^{-\lambda x} - \lambda x e^{-\lambda x}
	\end{align*}

	Then, to get the PDF, we differentiate our CDF, giving us the following:
	\begin{equation*}
		f_{t} = \lambda^{2} x e^{-\lambda x}, \quad t \geq 0
	\end{equation*}
\end{solution}

\end{document}