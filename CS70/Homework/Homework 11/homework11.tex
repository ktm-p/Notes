\documentclass{article}
%%%%%%% PREAMBLE %%%%%%%
%BEGIN_FOLD
%%%%% PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cabin} % section title font
\usepackage[default]{cantarell} % default font
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[scr]{rsfso} % power set symbol
\usepackage{tasks} % vaguely remember this being important for something...?
\usepackage{tikz} % diagrams
\usepackage{titlesec}
\usepackage{thmtools}
\usepackage{varwidth}
\usepackage{verbatim} % longer comments
\usepackage{xcolor}
%%%%%

%%%%% COLOURS
\definecolor{darkgreen}{HTML}{19A514}
\definecolor{lightgreen}{HTML}{9DFF9A}
\definecolor{darkblue}{HTML}{3E5FE4}
\definecolor{lightblue}{HTML}{BCDEFF}
\definecolor{darkred}{HTML}{CC3333}
\definecolor{lightred}{HTML}{FFA9A9}
\definecolor{darkpurple}{HTML}{A933CD}
\definecolor{lightpurple}{HTML}{F0BAFF}
\definecolor{darkyellow}{HTML}{D2D22A}
\definecolor{lightyellow}{HTML}{FFFFAE}
\definecolor{hyperlinkblue}{HTML}{3366CC}
%%%%%

%%%%% PAGE SETUP
% BASIC %
\setlength\parindent{0pt} % paragraph indentation
\setlength{\parskip}{5pt} % spacing between paragraphs
\usepackage[margin=1in]{geometry} % margin size

% HEADER/FOOTER %
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[R]{\thepage} % page number on bottom right
\fancyhead[R]{\textit{\leftmark}} % section title
\renewcommand{\headrulewidth}{0pt} % removing horizontal line at the top

% HYPERLINK FORMATTING %
\hypersetup{
	colorlinks,    
	linkcolor=hyperlinkblue,
	urlcolor=hyperlinkblue,
	pdftitle={...},
	pdfauthor={Michael Pham},
}

%%%%%

%%%%% ENVIRONMENTS STYLES
% SOLUTION ENVIRONMENT %
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

% PURPLE BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightpurple,
	linecolor=darkpurple,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkpurple}
]{purplebox}

% GREEN BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightgreen,
	linecolor=darkgreen,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkgreen}
]{greenbox}

% YELLOW BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightyellow,
	linecolor=darkyellow,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkyellow}
]{yellowbox}

% BLUE BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightblue,
	linecolor=darkblue,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkblue}
]{bluebox}

% RED BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightred,
	linecolor=darkred,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkred}
]{redbox}
%%%%%

%%%%% ENVIRONMENTS
% PURPLE BOXES (theorems, propositions, lemmas, and corollaries) %
\declaretheorem[style=purplebox,name=Theorem,within=section]{thm}
\declaretheorem[style=purplebox,name=Theorem,sibling=thm]{theorem}
\declaretheorem[style=purplebox,name=Theorem,numbered=no]{thm*, theorem*}
\declaretheorem[style=purplebox,name=Proposition,sibling=thm]{prop, proposition}
\declaretheorem[style=purplebox,name=Proposition,numbered=no]{prop*, proposition*}
\declaretheorem[style=purplebox,name=Lemma,sibling=thm]{lem, lemma}
\declaretheorem[style=purplebox,name=Lemma,numbered=no]{lem*, lemma*}
\declaretheorem[style=purplebox,name=Corollary,sibling=thm]{cor, corollary}
\declaretheorem[style=purplebox,name=Corollary,numbered=no]{cor*, corollary*}

% GREEN BOXES (definitions) %
\declaretheorem[style=greenbox,name=Definition,sibling=thm]{definition, defn}
\declaretheorem[style=greenbox,name=Definition,numbered=no]{definition*, defn*}

% BLUE BOXES (problems) %
\declaretheorem[style=bluebox,name=Problem,numberwithin=section]{homework, hw}
\declaretheorem[style=bluebox,name=Problem,numbered=no]{homework*, hw*}

% RED BOXES %
\declaretheorem[style=redbox,name=Remark,sibling=thm]{remark, rmk}
\declaretheorem[style=redbox,name=Remark, numbered=no]{remark*, rmk*}
\declaretheorem[style=yellowbox,name=Warning,sibling=thm]{warn}
\declaretheorem[style=yellowbox,name=Warning,numbered=no]{warn*}
%%%%%

%%%%% PROOF FORMATTING
\renewcommand\qedsymbol{$\blacksquare$}
%%%%%

%%% CUSTOM COMMANDS
% basic %
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}

% logic %
\newcommand*\xor{\oplus}
\newcommand{\all}{\forall}
\newcommand{\bland}{\bigwedge}
\newcommand{\blor}{\bigvee}
\newcommand*{\defeq}{\mathrel{\rlap{\raisebox{0.3ex}{$\m@th\cdot$}}\raisebox{-0.3ex}{$\m@th\cdot$}}=} \makeatother

% matrices %
\newcommand\aug{\fboxsep=- \fboxrule\!\!\!\fbox{\strut}\!\!\!}\makeatletter 

% sets %
\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

% title %
\newcommand{\mytitle}[2]{%
	\title{#1}
	\author{Michael Pham}
	\date{#2}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
}
%%%

%%% REDEFINING COMMANDS
\let\oldint\int
\renewcommand{\int}[2]{\oldint\limits_{#1}^{#2}}
\let\oldprod\prod
\renewcommand{\prod}[2]{\oldprod\limits_{#1}^{#2}}
\let\oldsum\sum
\renewcommand{\sum}[2]{\oldsum\limits_{#1}^{#2}}
%%%
%%%%%
%END_FOLD
%%%%%

\begin{document}
\mytitle{CS70 Homework 11}{Spring 2023}

\section{Maybe Lossy Maybe Not}
Let us say that Alice would like to send a message to Bob, over some channel. Alice has a message of length 6.
\begin{hw}
	Packets are dropped with probability $p$. If Alice sends 7 packets, what is the probability that Bob can successfully reconstruct Alice's message using polynomial interpolation.
\end{hw}
\begin{solution}
	We observe that in order for Bob to reconstruct a message of length 6, he'll need at least 6 packets; in other words, one of the packets can be dropped at most.
	
	Then, we see that the probability that Bob can reconstruct Alice's message is $\binom{7}{1}p(1-p)^{6} + (1-p)^{7}$.
\end{solution}

\begin{hw}
	Again, packets can be dropped with probability $p$. The channel may additionally corrupt 1 packet after deleting packets. Alice realizes this and sends 10 packets for a message of length 6. What is the probability that Bob receives enough packets to successfully reconstruct Alice's message using Berlekamp-Welch?
\end{hw}
\begin{solution}
	We observe that in this scenario, since there can be at most one corruption, then in order to guarantee that Bob can receive enough packets to always reconstruct Alice's message, then he needs at least eight packets (i.e. at most two packets are dropped).
	
	The probability of this happening is:
	\begin{equation*}
		\binom{10}{2}p^{2}(1-p)^{8} + \binom{10}{1}p(1-p)^{9} + \binom{10}{0}(1-p)^{10}
	\end{equation*}
\end{solution}

\begin{hw}
	Again, packets can be dropped with probability $p$. This time, packets may be corrupted with
	probability $q$. A packet being dropped is independent of whether or not it is corrupted (i.e. a
	packet may be both corrupted and dropped). Consider the original scenario where Alice sends
	7 packets for a message of length 6. What is probability that Bob can correctly reconstruct
	Aliceâ€™s message using polynomial interpolation on all of the points he receives?
\end{hw}
\begin{solution}
	For this, we observe that there are two cases.
	
	First, assuming that none of the packets are corrupted, then Bob can reconstruct the message if at most one packet is dropped. In other words, this scenario has a probability of $(1-q)^{7}( (1-p)^{7} + 7p(1-p)^{6})$.
	
	In the second scenario, the corrupted packet gets dropped as well. In this case, we see that the probability is $7q(1-q)^{6}p(1-p)^{6}$.
	
	Thus, the total probability is $(1-q)^{7}( (1-p)^{7} + 7p(1-p)^{6}) + 7q(1-q)^{6}p(1-p)^{6}$.
\end{solution}

\newpage

\section{Class Enrollment}
Lydia has just started her CalCentral enrollment appointment. She needs to register for a geography class and a history class. There are no waitlists, and she can attempt to enroll once per day in either class or both. The CalCentral enrollment system is strange and picky, so the probability of enrolling successfully in the geography class on each attempt is $p_{g}$ and the probability of enrolling successfully in the history class on each attempt is $p_{h}$. Also, these events are independent.

\begin{hw}
	Suppose Lydia begins by attempting to enroll in the geography class everyday and gets enrolled in it on day $G$. What is the distribution of $G$?
\end{hw}
\begin{solution}
	We observe that the distribution of $G$ is a geometric distribution, where $\Pr[G = n] = p_{g}(1-p_{g})^{n-1}$, where $n$ is the day on which Lydia gets enrolled.
\end{solution}

\begin{hw}
	Suppose she is not enrolled in the geography class after attempting each day for the first 7
	days. What is $\Pr[G = i \mid G > 7]$, the conditional distribution of $G$ given $G > 7$?
\end{hw}
\begin{solution}
	For this, we observe that $\Pr[G = i \mid G > 7] = \Pr[G = i - 7] = p_{g}(1-p_{g})^{i-8}$.
\end{solution}

\begin{hw}
	Once she is enrolled in the geography class, she starts attempting to enroll in the history class from day $G + 1$ and gets enrolled in it on day $H$. Find the expected number of days it takes Lydia to enroll in both the classes, i.e. $\mathbb{E}[H]$.
\end{hw}
\begin{solution}
	We observe that Lydia is expected to enroll in her geography class after $\frac{1}{p_{g}}$ days, and after that she is expected to enroll in her history class after an additional $\frac{1}{p_{h}}$ days. In other words, $\mathbb{E}[H] = \frac{1}{p_{g}} + \frac{1}{p_{h}}$.
\end{solution}

Suppose instead of attempting one by one, Lydia decides to attempt enrolling in both the classes from day 1. Let $G$ be the number of days it takes to enroll in the geography class, and $H$ be the number of days it takes to enroll in the history class.
\begin{hw}
	What is the distribution of $G$ and $H$ now? Are they independent?
\end{hw}
\begin{solution}
	The distribution of $G$ and $H$ are $\Pr[G=n] = p_{g}(1-p_{g})^{n-1}$ and $\Pr[H=n] = p_{h}(1-p_{h})^{n-1}$. They are independent.
\end{solution}

\begin{hw}
	Let $A$ denote the day she gets enrolled in her first class and let $B$ denote the day she gets enrolled in both the classes. What is the distribution of $A$?
\end{hw}
\begin{solution}
	We observe here that $A$ is essentially when either $G$ or $H$ occurs; from here, we can think of it in the following way: for each ``step", we see that the probability of either of the events occurring is $1-(1-p_{h})(1-p_{g})$. Then, we see that $\Pr[A = n] = \left( (1-p_{h})(1-p_{g}) \right)^{n-1}\left( 1-(1-p_{h})(1-p_{g}) \right)$.
\end{solution}

\begin{hw}
	What is the expected number of days it takes Lydia to enroll in both classes now, i.e. $\mathbb{E}[B]$?
\end{hw}
\begin{solution}
	We observe that $\Pr[B = n] = \Pr[G < n]\Pr[H = n] + \Pr[H < n]\Pr[G = n] + \Pr[G = n]\Pr[H = n]$.
	
	From here, we can rewrite it as a summation, which is:
	\begin{equation*}
		\Pr[B = n] = \left( \sum{i=1}{n-1}(1-p_{g})^{i-1}p_{g} \right)\left( (1-p_{h})^{n-1} p_{h} \right) + \left( \sum{i=1}{n-1}(1-p_{h})^{i-1}p_{h} \right)\left( (1-p_{g})^{n-1} p_{g} \right) + (1-p_{h})^{n-1}p_{h}(1-p_{g})^{n-1}p_{g}
	\end{equation*}

	Now, we see that $\mathbb{E}[B] = \sum{n=1}{\infty} k \Pr[B = n]$. This expression simplifies down to being:
	\begin{equation*}
		\mathbb{E}[B] = \dfrac{1}{p_{h}} + \dfrac{1}{p_{g}} - \dfrac{1}{p_{h}+p_{g}-p_{g}p_{h}}
	\end{equation*}

	A different way to think of the final answer is that we're finding $\mathbb{E}[G] + \mathbb{E}[H] - \mathbb{E}[H \cup G]$.
\end{solution}

\begin{hw}
	What is the expected number of classes she will be enrolled in by the end of 30 days?
\end{hw}
\begin{solution}
	We observe that the expected number of classes that she'll be enrolled in at the end of 30 days is:
	\begin{equation*}
		\sum{i=1}{30}(1-p_{g})^{i-1}(p_{g})(1-p_{h})^{30} + \sum{i=1}{30}(1-p_{h})^{i-1}(p_{h})(1-p_{g})^{30} + 2\sum{i=1}{30}\sum{j=1}{30}(1-p_{g})^{i-1}p_{g}(1-p_{h})^{j-1}p_{h}
	\end{equation*}
\end{solution}

\newpage

\section{Swaps and Cycles}
We'll say that a permutation $\pi = (\pi(1),\ldots,\pi(n))$ contains a \emph{swap} if there exist $i,j\in\{1,\ldots,n\}$ so that $\pi(i) = j$ and $\pi(j) = i$, where $i \neq j$. 

\begin{hw}
	What is the expected number of swaps in a random permutation?
\end{hw}
\begin{solution}
	To begin with, let $X_{i} = 1$ if the $i^{th}$ number is in a swap, and $X_{i} = 0$ otherwise. Now, we denote $S$ to be the number of switches. We see then that:
	\begin{equation*}
		S = \dfrac{X_{1} + \ldots + X_{n}}{2}
	\end{equation*}

	Now, from here, we want to find $\mathbb{E}[S]$. To do this, we observe the following:
	\begin{align*}
		\mathbb{E}[S] &= \mathbb{E}\left[ \dfrac{X_{1} + \ldots + X_{n}}{2} \right] \\
		&= \dfrac{1}{2}\mathbb{E}[X_{1} + \ldots + X_{n}] \\
		&= \dfrac{n}{2}\mathbb{E}[X_{1}]
	\end{align*}

	Now, in order to find $\mathbb{E}[X_{1}]$, we can first find $\Pr[1 \text{ is in a switch with 2}]$, then multiply by $n-1$. We see then that there are $n!$ ways to order the numbers, and out of those $n!$ ways, $(n-2)!$ of them are where 1 and 2 are swapped. Thus, we have that $\frac{(n-2)!}{n!} = \frac{1}{n(n-1)}$.
	
	Thus, our final value for $\mathbb{E}[S]$ is:
	\begin{align*}
		\mathbb{E}[S] &= \dfrac{n}{2}(n-1)\left( \dfrac{1}{n(n-1)} \right) \\
		&= \dfrac{1}{2}
	\end{align*}
\end{solution}

\begin{hw}
	What about the variance?
\end{hw}
\begin{solution}
	We observe that $\mathrm{Var}[S] = \mathbb{E}[S^{2}] - \mathbb{E}[S]^{2}$.
	
	We know that $\mathbb{E}[S]^{2} = \frac{1}{4}$. Now, to find $\mathbb{E}[S^{2}]$, we first see that:
	\begin{align*}
		S &= \dfrac{1}{2}(X_{1} + \ldots + X_{n}) \\
		S^{2} &= \dfrac{1}{4}(X_{1} + \ldots + X_{n})^{2} \\
		\mathbb{E}[S^{2}] &= \dfrac{1}{4}\left( \sum{i=1}{n} \mathbb{E}[X_{i}^{2}] + 2 \sum{i<j}{} \mathbb{E}[X_{i}X_{j}] \right)
	\end{align*}

	Now, we know that $\sum{i=1}{n} \mathbb{E}[X_{i}^{2}] = n\left( \frac{1}{n} \right) = 1$. In order to find $\mathbb{E}[X_{i}X_{j}]$, we see that there are two cases:
	\begin{enumerate}
		\item $X_{i}$ and $X_{j}$ are in the same swap. In this case, the probability is $\frac{1}{n(n-1)}$ from the last part.
		
		\item $X_{i}$ and $X_{j}$ are in different swaps. In this case, we see first we have $\frac{n-2}{n}$ choices, then second we have $\frac{1}{n-1}$ choice. The third pick, we have $\frac{n-3}{n-2}$ choices, and the fourth we have $\frac{1}{n-3}$ choices. Thus, the total probability is $\frac{(n-2)(1)(n-3)(1)}{n(n-1)(n-2)(n-3)} = \frac{1}{n(n-1)}$
	\end{enumerate}

	Therefore, we see then that the probability that $X_{i}$ and $X_{j}$ are both in a swap is equal to $\frac{1}{n(n-1)} + \frac{1}{n(n-1)} = \frac{2}{n(n-1)}$.
	
	With this in mind then, we see the following:
	\begin{align*}
		\mathbb{E}[S^{2}] &= \dfrac{1}{4}\left( 1 + 2\binom{n}{2}\dfrac{2}{n(n-1)} \right) \\
		&= \dfrac{1}{4}\left( 1 + 2 \right) \\
		&= \dfrac{3}{4}
	\end{align*}

	And with that, we have that $\mathrm{Var}[S] = \frac{3}{4} - \frac{1}{4} = \frac{1}{2}$.
\end{solution}

\begin{hw}
	In the same spirit as above, we'll say that $\pi$ contains a \emph{$k$-cycle} if there exist $i_1,\ldots,i_k \in \{1,\ldots,n\}$ with $\pi(i_1) = i_2,\pi(i_2) = i_3,\ldots,\pi(i_k) = i_1$. Compute the expectation of the number of $k$-cycles. 
\end{hw}
\begin{solution}
	In order to find the expectation of the number of $k$-cycles, $\mathbb{E}[S_{k}]$, we observe that we have $\binom{n}{k}$ tuples to form a $k$-cycle. 
	
	From here, we see that the probability of forming a $k$-cycle is:
	\begin{equation*}
		\dfrac{k-1}{n} \cdot \dfrac{k-2}{n-1} \cdots = \dfrac{(k-1)!}{n!}
	\end{equation*}

	We then multiply this by $(n-k)!$ because the remaining numbers can be in any order. Putting this all together, we have then that $\mathbb{E}[S_{k}] = \binom{n}{k} \frac{(n-k)!(k-1)!}{n!} = \frac{1}{k}$.
\end{solution}

\newpage

\section{Throwing Frisbees}
Shahzar and his $n-1$ friends stand in a circle and play the following game: Shahzar throws a frisbee to one of the other people in the circle randomly, with each person being equally likely, and thereafter, the person holding the frisbee throws it to someone else in the circle, again uniformly at random. The game ends when someone throws the frisbee back to Shahzar.
\begin{hw}
	What is the expected number of times the frisbee is thrown through the course of the game?
\end{hw}
\begin{solution}
	We observe that after Shahzar has thrown the frisbee, we can think of the scenario as being like a geometric distribution, where it ends after reaching Shahzar again; this has a probability of $\frac{1}{n-1}$. Then, we see that the expected value $\mathbb{E}[S] = 1 + \frac{1}{\frac{1}{n-1}} = 1 + n-1 = n$
\end{solution}

\begin{hw}
	What is the expected number of people that never get the frisbee during the game?
\end{hw}
\begin{solution}
	Let $X_{i}$ denote that the $i^{th}$ person did not receive a frisbee. We see then that the total number of people who didn't receive it is $X = X_{1} + \ldots + X_{n-1}$.
	
	Now, from here, we observe the following:
	\begin{align*}
		\mathbb{E}[X] &= \mathbb{E}[X_{1} + \ldots + X_{n-1}] \\
		&= \mathbb{E}[X_{1}] + \ldots + \mathbb{E}[X_{n-1}]
	\end{align*}

	From here, we see that $\mathbb{E}[X_{i}] = \left( 1 - \frac{1}{n-1} \right)^{n}$. Then, we see that:
	\begin{align*}
		\mathbb{E}[X] &= \mathbb{E}[X_{1}] + \ldots + \mathbb{E}[X_{n-1}] \\
		&= (n-1)\mathbb{E}[X_{1}] \\
		&= (n-1)\left( 1 - \frac{1}{n-1} \right)^{n}
	\end{align*}
\end{solution}

\newpage

\section{Balls and Bins}
\begin{hw}
	Throw $n$ balls into $m$ bins, where $m$ and $n$ are positive integers. Let $X$ be the number of bins with exactly one ball. Compute $\mathrm{Var} [X]$. Your final answer should not contain any summations.
\end{hw}
\begin{solution}
	First, we will want to find $\mathbb{E}[X]$. To do this, we can let $X_{i} = 1$ if the $i^{th}$ bin has one ball in it, and $X_{i} = 0$ otherwise.
	
	Now, let $X = X_{1} + \ldots + X_{m}$, where $X$ is the number of bins that have exactly one ball. Then, in order to find $\mathbb{E}[X]$, we do the following:
	\begin{align*}
		\mathbb{E}[X] &= \mathbb{E}[X_{1} + \ldots + X_{m}] \\
		&= m\mathbb{E}[X_{1}]
	\end{align*}

	Now, to find $\mathbb{E}[X_{1}]$, we observe that we want one ball to go into the bin, and the rest go to other bins. The probability of this happening is $n \left( \frac{1}{m} \right)\left( \frac{m-1}{m} \right)^{n-1}$.
	
	Therefore, we have that $\mathbb{E}[X] = m(n)\left(\frac{1}{m} \right)\left( \frac{m-1}{m} \right)^{n-1} = n \left( \frac{m-1}{m} \right)^{n-1}$.
	
	Now, to find $\mathrm{Var}[X]$, we observe that $\mathrm{Var}[X] = \mathbb{E}[X^{2}] - \mathbb{E}[X]^{2}$.
	
	To find $\mathbb{E}[X]^{2}$, we observe that this is simply $m(n) \left( \frac{1}{m} \right)\left( \frac{m-1}{m} \right)^{n-1} = n \left( \frac{m-1}{m} \right)^{n-1}$
	
	To find $\mathbb{E}[X^{2}]$, we first see that:
	\begin{align*}
		X &= X_{1} + \ldots + X_{m} \\
		X^{2} &= (X_{1} + \ldots + X_{m})^{2} \\
		\mathbb{E}[X^{2}] &= \sum{i=1}{m}\mathbb{E}[X_{i}^{2}] + 2\sum{i<j}{}\mathbb{E}[X_{i}X_{j}]
	\end{align*}

	In order to find $\mathbb{E}[X_{i}X_{j}]$, we want to find the probability that both $X_{i}$ and $X_{j}$ have one ball in them. We see that the probability of this happening is $\binom{n}{2}(\frac{2}{m})(\frac{1}{m})\left( \frac{m-2}{m} \right)^{n-2}$.
	
	Then, from here, we see that:
	\begin{align*}
		2\sum{i<j}{} \mathbb{E}[X_{i}X_{j}] &= 2\binom{m}{2}\binom{n}{2}\left( \dfrac{2}{m} \right)\left( \dfrac{1}{m} \right)\left( \dfrac{m-2}{m} \right)^{n-2} \\
		&= 4 \binom{m}{2} \binom{n}{2} \left( \dfrac{1}{m} \right)^{2}\left( \dfrac{m-2}{m} \right)^{n-2}
	\end{align*}

	Putting it all together, we get the following:
	\begin{equation*}
		\mathrm{Var}[X] = n\left( \dfrac{m-1}{m} \right)^{n-1} + 4 \binom{m}{2} \binom{n}{2} \left( \dfrac{1}{m} \right)^{2}\left( \dfrac{m-2}{m} \right)^{n-2} - n^{2}\left( \dfrac{m-1}{m} \right)^{2n-2}
	\end{equation*}
\end{solution}

\newpage

\section{Will I Get My Package?}
A delivery guy in some company is out delivering $n$ packages to $n$ customers, where $n$ is a natural number greater than 1.
Not only does he hand each customer a package uniformly at random from the remaining packages, he opens the package before delivering it with probability $1/2$.
Let $X$ be the number of customers who receive their own packages unopened. 

\begin{hw}
	Compute the expectation $\mathbb{E}[X]$.
\end{hw}
\begin{solution}
	Let $X_{i} = 1$ if the $i^{th}$ person receives their own package unopened, and 0 otherwise. From here, we see that $X = X_{1} + \ldots X_{n}$.
	
	Then, we observe that:
	\begin{align*}
		\mathbb{E}[X] &= \mathbb{E}[X_{1} + \ldots + X_{n}] \\
		&= n\mathbb{E}[X_{1}]
	\end{align*}

	From here, we observe that the probability a person receives their own package is $\frac{1}{n}$, and the probability that it's unopened is $\frac{1}{2}$. Then, with this in mind, we see that:
	\begin{align*}
		\mathbb{E}[X] &= n\left( \frac{1}{2n} \right) \\
		&= \frac{1}{2}
	\end{align*}
\end{solution}

\begin{hw}
	Compute the variance $\mathrm{Var}[X]$.
\end{hw}
\begin{solution}
	Now, we want to compute the variance. Again, recall that $\mathrm{Var}[X] = \mathbb{E}[X^{2}] - \mathbb{E}[X]^{2}$.
	
	Now, we want to find $\mathbb{E}[X^{2}]$. Observe first that:
	\begin{align*}
		X &= X_{1} + \ldots + X_{n} \\
		X^{2} &= (X_{1} + \ldots + X_{n})^{2} \\
		\mathbb{E}[X^{2}] &= \sum{i=1}{n}\mathbb{E}[X_{i}^{2}] + 2\sum{i<j}{}\mathbb{E}[X_{i}X_{j}]
	\end{align*}

	We first observe that $\sum{i=1}{n}\mathbb{E}[X_{i}^{2}] = \frac{1}{2}$. Now, in order to find $\Pr[X_{i} = 1 \cap X_{j} = 1]$, we find the probability that both $i$ and $j$ receive their own packages and they're both unopened:
	\begin{align*}
		\Pr[X_{i} = 1 \cap X_{j} = 1] &= \dfrac{1}{2}\cdot\dfrac{1}{n}\cdot\dfrac{1}{2}\cdot\dfrac{1}{n-1} \\
		&= \dfrac{1}{4n(n-1)}
	\end{align*}

	Therefore, we see then that:
	\begin{align*}
		2\sum{i<j}{}\mathbb{E}[X_{i}X_{j}] &= 2\binom{n}{2}\left( \dfrac{1}{4n(n-1)} \right) \\
		&= \dfrac{1}{4}
	\end{align*}

	With this, we observe that $\mathbb{E}[X^{2}] = \frac{1}{2} + \frac{1}{4} = \frac{3}{4}$.
	
	Putting it all together, we have the following:
	\begin{align*}
		\mathrm{Var}[X] &= \mathbb{E}[X^{2}] - \mathbb{E}[X]^{2} \\
		&= \dfrac{3}{4} - \dfrac{1}{4} \\
		&= \dfrac{1}{2}
	\end{align*}
\end{solution}
	
%	We can think of this as a binomial distribution situation, where we have $n$ trials (since we throw $n$ balls), where we define the probability of success $p$ as being the ball landing in a bin other than the one we want. We observe then that we have $p = (1 - \frac{1}{m})^{n-1}$.
%	
%	Now, we observe that the expected value of a binomial distribution is $n p = n \left( \frac{m-1}{m} \right)^{n-1}$.
%	
%	To find the variance, we observe that  $\mathrm{Var}(X) = n \left(\frac{m-1}{m}\right)^{n-1} \left[\frac{n(m-1)}{m} - \frac{(n-1)(m-2)}{m^2}\right]$.

\end{document}