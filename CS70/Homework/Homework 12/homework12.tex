\documentclass{article}
%%%%%%% PREAMBLE %%%%%%%
%BEGIN_FOLD
%%%%% PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cabin} % section title font
\usepackage[default]{cantarell} % default font
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[scr]{rsfso} % power set symbol
\usepackage{tasks} % vaguely remember this being important for something...?
\usepackage{tikz} % diagrams
\usepackage{titlesec}
\usepackage{thmtools}
\usepackage{varwidth}
\usepackage{verbatim} % longer comments
\usepackage{xcolor}
%%%%%

%%%%% COLOURS
\definecolor{darkgreen}{HTML}{19A514}
\definecolor{lightgreen}{HTML}{9DFF9A}
\definecolor{darkblue}{HTML}{3E5FE4}
\definecolor{lightblue}{HTML}{BCDEFF}
\definecolor{darkred}{HTML}{CC3333}
\definecolor{lightred}{HTML}{FFA9A9}
\definecolor{darkpurple}{HTML}{A933CD}
\definecolor{lightpurple}{HTML}{F0BAFF}
\definecolor{darkyellow}{HTML}{D2D22A}
\definecolor{lightyellow}{HTML}{FFFFAE}
\definecolor{hyperlinkblue}{HTML}{3366CC}
%%%%%

%%%%% PAGE SETUP
% BASIC %
\setlength\parindent{0pt} % paragraph indentation
\setlength{\parskip}{5pt} % spacing between paragraphs
\usepackage[margin=1in]{geometry} % margin size

% HEADER/FOOTER %
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[R]{\thepage} % page number on bottom right
\fancyhead[R]{\textit{\leftmark}} % section title
\renewcommand{\headrulewidth}{0pt} % removing horizontal line at the top

% HYPERLINK FORMATTING %
\hypersetup{
	colorlinks,    
	linkcolor=hyperlinkblue,
	urlcolor=hyperlinkblue,
	pdftitle={...},
	pdfauthor={Michael Pham},
}

%%%%%

%%%%% ENVIRONMENTS STYLES
% SOLUTION ENVIRONMENT %
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

% PURPLE BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightpurple,
	linecolor=darkpurple,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkpurple}
]{purplebox}

% GREEN BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightgreen,
	linecolor=darkgreen,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkgreen}
]{greenbox}

% YELLOW BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightyellow,
	linecolor=darkyellow,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkyellow}
]{yellowbox}

% BLUE BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightblue,
	linecolor=darkblue,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkblue}
]{bluebox}

% RED BOX %
\declaretheoremstyle[
mdframed={
	backgroundcolor=lightred,
	linecolor=darkred,
	rightline=false,
	topline=false,
	bottomline=false,
	linewidth=2pt,
	innertopmargin=8pt,
	innerbottommargin=8pt,
	innerleftmargin=8pt,
	leftmargin=-2pt,
	skipbelow=2pt,
	nobreak
},
headfont=\normalfont\bfseries\color{darkred}
]{redbox}
%%%%%

%%%%% ENVIRONMENTS
% PURPLE BOXES (theorems, propositions, lemmas, and corollaries) %
\declaretheorem[style=purplebox,name=Theorem,within=section]{thm}
\declaretheorem[style=purplebox,name=Theorem,sibling=thm]{theorem}
\declaretheorem[style=purplebox,name=Theorem,numbered=no]{thm*, theorem*}
\declaretheorem[style=purplebox,name=Proposition,sibling=thm]{prop, proposition}
\declaretheorem[style=purplebox,name=Proposition,numbered=no]{prop*, proposition*}
\declaretheorem[style=purplebox,name=Lemma,sibling=thm]{lem, lemma}
\declaretheorem[style=purplebox,name=Lemma,numbered=no]{lem*, lemma*}
\declaretheorem[style=purplebox,name=Corollary,sibling=thm]{cor, corollary}
\declaretheorem[style=purplebox,name=Corollary,numbered=no]{cor*, corollary*}

% GREEN BOXES (definitions) %
\declaretheorem[style=greenbox,name=Definition,sibling=thm]{definition, defn}
\declaretheorem[style=greenbox,name=Definition,numbered=no]{definition*, defn*}

% BLUE BOXES (problems) %
\declaretheorem[style=bluebox,name=Problem,numberwithin=section]{homework, hw}
\declaretheorem[style=bluebox,name=Problem,numbered=no]{homework*, hw*}

% RED BOXES %
\declaretheorem[style=redbox,name=Remark,sibling=thm]{remark, rmk}
\declaretheorem[style=redbox,name=Remark, numbered=no]{remark*, rmk*}
\declaretheorem[style=yellowbox,name=Warning,sibling=thm]{warn}
\declaretheorem[style=yellowbox,name=Warning,numbered=no]{warn*}
%%%%%

%%%%% PROOF FORMATTING
\renewcommand\qedsymbol{$\blacksquare$}
%%%%%

%%% CUSTOM COMMANDS
% basic %
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}

% logic %
\newcommand*\xor{\oplus}
\newcommand{\all}{\forall}
\newcommand{\bland}{\bigwedge}
\newcommand{\blor}{\bigvee}
\newcommand*{\defeq}{\mathrel{\rlap{\raisebox{0.3ex}{$\m@th\cdot$}}\raisebox{-0.3ex}{$\m@th\cdot$}}=} \makeatother

% matrices %
\newcommand\aug{\fboxsep=- \fboxrule\!\!\!\fbox{\strut}\!\!\!}\makeatletter 

% sets %
\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

% probability stuff %
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\corr}{\mathrm{Corr}}
% title %
\newcommand{\mytitle}[2]{%
	\title{#1}
	\author{Michael Pham}
	\date{#2}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
}
%%%

%%% REDEFINING COMMANDS
\let\oldint\int
\renewcommand{\int}[2]{\oldint\limits_{#1}^{#2}}
\let\oldprod\prod
\renewcommand{\prod}[2]{\oldprod\limits_{#1}^{#2}}
\let\oldsum\sum
\renewcommand{\sum}[2]{\oldsum\limits_{#1}^{#2}}
%%%
%%%%%
%END_FOLD
%%%%%

\begin{document}
\mytitle{Homework 12}{Spring 2023}

\section{Double-Check Your Intuition Again}
For the first two questions, suppose that you roll a fair six-sided die and record the result $X$. You roll the die again and record the result $Y$.
\begin{hw}
	What is $\cov (X+Y, X-Y)$?
\end{hw}
\begin{solution}
	We observe that $\cov(X+Y, X-Y) = \E[(X+Y)(X-Y)]- \E[X+Y]\E[X-Y]$.
	
	Now, observe that $(X+Y)(X-Y) = X^{2} - Y^{2}$, which yields us $\E[(X+Y)(X-Y)] = \E[X^{2} - Y^{2}] = \E[X^{2}] - \E[Y^{2}]$.
	
	By Linearity of Expectations, we have that $\E[X+Y]\E[X-Y] = \left( \E[X] + \E[Y] \right)\left( \E[X] - \E[Y] \right) = \E[X]^{2} - \E[Y]^{2}$.
	
	We see then that:
	\begin{align*}
		\cov (X+Y, X-Y) &= (\E[X^{2}] - \E[Y^{2}]) - (\E[X]^{2} - \E[Y]^{2}) \\
		&= (\E[X^{2}] - \E[X]^{2}) - (\E[Y^{2}] - \E[Y]^{2}) \\
		&= \Var(X) - \Var(Y)
	\end{align*}

	Notice here that since $X$ and $Y$ are independent, identically distributed random variables, we have that $\Var(X) = \Var(Y)$. Therefore, we see that $\cov(X+Y, X-Y) = 0$
\end{solution}

\begin{hw}
	Prove that $X+Y$ and $X-Y$ are not independent.
\end{hw}
\begin{solution}
	Since we find that $\cov(X+Y, X-Y) = 0$, we have to instead prove dependence with a different method.
	
	To do this, observe that if $X + Y = 12$, we know then that the only way for this to happen is if both $X,Y$ are 6; then, we know that $X-Y = 0$.
	
	From here, we observe the following:
	\begin{align*}
		\Pr[X-Y = 0 \mid X + Y = 12] &= 1 \\
		\Pr[X - Y = 0] &= \dfrac{6}{36} \\
		&= \dfrac{1}{6}
	\end{align*}

	We see then that since $\Pr[X-Y = 0 \mid X + Y = 12] \not= \Pr[X-Y = 0]$, it follows then that $X+Y$ and $X-Y$ are not independent.
\end{solution}

For each of the problems below, if you think the answer is "yes" then provide a proof. If you think the answer is "no", then provide a counterexample.
\begin{hw}
	If $X$ is a random variable and $\Var (X) = 0$, then must $X$ be a constant?
\end{hw}
\begin{solution}
	Yes.
	
	Suppose that $\Var(X) = 0$. Now, we observe that since $\Var(X) = \E[(X - \E[X])^{2}]$, we observe that the only way for $\Var(X) = 0$ is if $(X - \E[X])^{2} = 0$. Now, we observe that this means:
	\begin{align*}
		(X - \E[X])^{2} &= 0 \\
		X - \E[X] &= 0 \\
		X &= \E[X]
	\end{align*}

	The only way for $X = \E[X]$ to be the case is if $X$ is a constant.
\end{solution}

\begin{hw}
	If $X$ is a random variable and $c$ is a constant, then is $\Var (cX) = c \Var (X)$?
\end{hw}
\begin{solution}
	No.
	
	We recall that $\Var(X) = \E[X^{2}] + \E[X]^{2}$. Now, suppose that we roll a fair six-sided die and record its result $X$. We see then that $\E[X] = \frac{7}{2}$, $\E[X]^{2} = \frac{49}{4}$, and $\E[X^{2}] = \frac{91}{6}$. We have then that $\Var(X) = \frac{35}{12}$.
	
	Now, let $Y = 2X$. We see then that $\E[Y] = 7$, $\E[Y]^{2} = 49$, and $\E[Y^{2}] = \frac{182}{3}$. Then, we have $\Var(X) = \frac{35}{3} = 2^{2}\left( \frac{35}{12} \right)$. Thus, we see that $\Var(cX) = c^{2} \Var(X)$.
\end{solution}

\begin{hw}
	If $A$ and $B$ are random variables with nonzero standard deviations and $\corr (A, B) = 0$, then are $A$ and $B$ independent?
\end{hw}
\begin{solution}
	No.
	
	We can use the scenario given for Problem 1.1 and 1.2: suppose that you roll a fair six-sided die and record the result $X$. You roll the die again and record the result $Y$. 
	
	Now, we define the random variable $A = X + Y$, and $B = X - Y$. We observe that since $\cov(A,B) = 0$, it follows then that $\corr(A,B) = 0$. However, as we've established previously, $A$ and $B$ aren't independent.
\end{solution}

\begin{hw}
	If $X$ and $Y$ are not necessarily independent random variables, but $\corr (X, Y) = 0$, and $X$ and $Y$ have nonzero standard deviations, then is $\Var (X+Y) = \Var(X) + \Var(Y)$?
\end{hw}
\begin{solution}
	Yes.
	
	To begin with, recall the following:
	\begin{equation*}
		\corr(X,Y) = \dfrac{\cov(X,Y)}{\sigma_{x}\sigma_{y}}
	\end{equation*}
	where $\sigma$ denotes the standard deviation. Now, because $\sigma_{x}, \sigma_{y} > 0$, it follows then that for $\corr(X,Y) = 0$, we must have $\cov(X,Y) = 0$.
	
	Now, since $\Var(X+Y) = \Var(X) + \Var(Y) + 2\cov(X,Y)$, and we have that $\cov(X,Y) = 0$, it follows then that:
	\begin{align*}
		\Var(X+Y) &= \Var(X) + \Var(Y) + 2(0) \\
		&= \Var(X) + \Var(Y)
	\end{align*}
\end{solution}

\begin{hw}
	If $X$ and $Y$ are random variables then is $\E[\max (X, Y) \min (X, Y)] = \E[X Y]$?
\end{hw}
\begin{solution}
	Yes.
	
	We observe that $\max(X,Y) = \frac{X+Y+\lvert X-Y \rvert}{2}$, and $\min(X,Y) = \frac{X+Y-\lvert X-Y \rvert}{2}$.
	
	With this in mind, we see that:
	\begin{align*}
		\E[\max (X, Y) \min (X, Y)] &= \E\left[ \left( \frac{X+Y+\lvert X-Y \rvert}{2} \right) \left( \frac{X+Y-\lvert X-Y \rvert}{2} \right)\right] \\
		&= \dfrac{1}{4}\E[\left( X+Y+\lvert X-Y \rvert \right)\left( X+Y-\lvert X-Y \rvert \right)] \\
		&= \dfrac{1}{4}\E[\left( X+Y \right)^{2} - \left( X-Y \right)^{2}] \\
		&= \dfrac{1}{4}\E[4XY] \\
		&= \E[XY]
	\end{align*}
\end{solution}

\begin{hw}
	If $X$ and $Y$ are independent random variables with nonzero standard deviations, then is $$\corr (\max (X, Y), \min (X, Y)) = \corr (X, Y) ?$$
\end{hw}
\begin{solution}
	No.
	
	Suppose that we flip two fair coins. Let $X$ and $Y$ denote the following:
	\[
	X = 
	\begin{cases}
		1 & \text{if first coin lands on heads} \\
		0 & \text{if first coin lands on tails}
	\end{cases} \qquad
	Y =
	\begin{cases}
		0 & \text{if second coin lands on heads} \\
		1 & \text{if second coin lands on tails}
	\end{cases}
	\]
	
	We note here that $X$ and $Y$ are independent random variables. Now, with this in mind, we see that there are four possibilities for our coin flips: $\left\{  HH, HT, TH, TT\right\}$. From this, we see the following:
	\[
	\max(X,Y) = 
	\begin{cases}
		1 & \text{if not $TH$} \\
		0 & \text{if $TH$}
	\end{cases} \qquad
	\min(X,Y) =
	\begin{cases}
		0 & \text{if not $HT$} \\
		1 & \text{if $HT$}
	\end{cases}
	\]
	
	From here, we observe that $\E[\max(X,Y)] = \frac{3}{4}$, and $\E[\min(X,Y)] = \frac{1}{4}$.
	
	Furthermore, we observe that $\E[\max(X,Y)\min(X,Y)] = \E[XY]$ from the previous part. And since $X, Y$ are independent, then $\E[XY] = \E[X]\E[Y] = \frac{1}{4}$.
	
	Now, we observe that $\Var(\max(X,Y)) = \frac{3}{4}\left( 1 - \frac{3}{4} \right) = \frac{3}{16} = \Var(\min(X,Y))$. Then, we have that $\sigma_{x} = \sigma_{y} = \sqrt{\Var(\max(X,Y))} = \sqrt{\Var(\min(X,Y))} = \frac{\sqrt{3}}{4}$.
	
	From here, we notice the following:
	\begin{align*}
		\corr(\max(X,Y), \min(X,Y)) &= \dfrac{\cov(\max(X,Y), \min(X,Y))}{\sigma_x\sigma_y} \\
		&= \dfrac{\E[\max(X,Y)\min(X,Y)] - \E[\max(X,Y)]\E[\min(X,Y)]}{\sigma_x\sigma_y} \\
		&= \dfrac{\left( \dfrac{1}{4} \right) - \left( \dfrac{3}{4} \right)\left( \dfrac{1}{4} \right)}{\left( \dfrac{\sqrt{3}}{4} \right)\left( \dfrac{\sqrt{3}}{4} \right)} \\
		&= \dfrac{1}{3}
	\end{align*}

	However, we notice here that since $X,Y$ are independent, then it means that $\corr(X,Y) = 0 \not= \frac{1}{3} = \corr(\max(X,Y), \min(X,Y))$.
\end{solution}

\newpage

\section{Unreliable Servers}
A Google competitor owns a warehouse consisting of a very large number of servers (a server farm). On any given day, each server in the farm is equally likely to go down or to stay online, independently of all other servers, and independently of what happens on any number of other days. On average, 4 servers go down in the cluster per day.

\begin{hw}
	What is an appropriate distribution to model the number of servers that crash on any given day for a certain cluster? (Give the name and parameter(s) of the distribution.)
\end{hw}
\begin{solution}
	An appropriate distribution to use here would be the Poisson Distribution, which has parameter $\lambda = 4$ for our scenario. Thus, letting $X$ denote the number of servers that crash on a given day for a certain cluster, we have $\Pr[X = n] = \frac{4^{n}}{n!} e^{-4}$.
\end{solution}

\begin{hw}
	Compute the expected value and variance of the number of crashed servers on a given day for a certain cluster. 
\end{hw}
\begin{solution}
	Since this is a Poisson distribution, we see then that $\E[X] = \var[X] = \lambda = 4$.
\end{solution}

\begin{hw}
	Compute the probability that strictly less than 3 servers crashed on a given day for a certain cluster. 
\end{hw}
\begin{solution}
	The probability that $\Pr[X < 3] = \Pr[X = 0] + \Pr[X = 1] + \Pr[X = 2]$. We see that this is equal to:
	\begin{align*}
		\frac{4^{0}}{0!} e^{-4} + \frac{4^{1}}{1!} e^{-4} + \frac{4^{2}}{2!} e^{-4} = 13e^{-4}
	\end{align*}
\end{solution}

\begin{hw}
	Compute the probability that at least 3 servers crashed on a given day for a certain cluster.
\end{hw}
\begin{solution}
	We observe that $\Pr[X \geq 3] = 1 - \Pr[X < 3] = 1 - 13e^{-4}$.
\end{solution}

\begin{hw}
	Compute the probability that exactly 6 servers crashed over a given two-day period for a certain cluster.
\end{hw}
\begin{solution}
	Because we're looking at a two-day period instead of a one-day period, then letting $X([1,2])$ denote the number of servers that crash in a two-day period, we have $\Pr[X([1,2]) = n] = \frac{8^{n}}{n!} e^{-8}$.
	
	Then, we observe that $\Pr[X([1,2]) = 6] = \frac{8^{6}}{6!} e^{-8} = \frac{16384}{45}e^{-8}$.
\end{solution}

\newpage

\section{Geometric and Poisson}
\begin{hw}
	Let $X \sim \mathrm{Geometric}(p)$ and $Y\sim \mathrm{Poisson}(\lambda)$ be independent random variables. Compute $\Pr[X>Y]$. Your final answer should not have summations.
\end{hw}
\begin{solution}
	To begin with, we observe that $\Pr[X>Y]$ is equivalent to saying:
	\begin{align*}
		\Pr[X>Y] &= \sum{n=1}{\infty} \Pr[X > n, Y = n] \\
		&=\sum{n=1}{\infty}\Pr[X > n]\Pr[Y = n]
	\end{align*} where the last equality follows from the fact that $X,Y$ are independent variables. 
	
	From here, we see that since $Y$ is a Poisson random variable, then $\Pr[Y=n] = \frac{\lambda^{n}}{n!}e^{-\lambda}$.
	
	Meanwhile, we have that $\Pr[X > n] = 1 - \Pr[X \leq n]$. From here, we observe the following:
	
	
	Now, in order to find $\Pr[X > n]$, we first note that $X$ is a geometric random variable. Now, we note that we note that $\Pr[X > n]$ is essentially asking us what's the probability of getting at least $n$ failures. Thus, we can rewrite it as a sum as follows:
	\begin{align*}
		\Pr[X > n] &= p \sum{i=n}{\infty} (1-p)^{n}
	\end{align*}

	We now observe that $\sum{i=n}{\infty} (1-p)^{n}$ is asking us to find the sum of a geometric series; from here, we see that the first term is $(1-p)^{n}$, and the ratio is $(1-p)$. With this, we get:
	\begin{align*}
		p \sum{i=n}{\infty} (1-p)^{n} &= \dfrac{p(1-p)^{n}}{1-(1-p)} \\
		&= \dfrac{p(1-p)^{n}}{p} \\
		&= (1-p)^{n}
	\end{align*}

	Now, with these two pieces, we can then find what $\Pr[X > Y]$ is:
	\begin{align*}
		\sum{n=1}{\infty} \Pr[X > n] \Pr[Y = n] &= \sum{n=1}{\infty} (1-p)^{n} \dfrac{\lambda^{n}}{n!} e^{-\lambda} \\
		&= e^{-\lambda} \sum{n=1}{\infty} \dfrac{(1-p)^{n}\lambda^{n}}{n!} \\
		&= e^{-\lambda} \sum{n=1}{\infty} \dfrac{(\lambda(1-p))^{n}}{n!} \\
		&= e^{-\lambda}e^{\lambda(1-p)} \tag*{by Taylor Series definition of $e$} \\
		&= e^{-\lambda p}
	\end{align*}
\end{solution}

\newpage

\section{Coupon Collector Variance}
It's that time of the year again---Safeway is offering its Monopoly Card promotion. Each time you visit Safeway, you are given one of $n$ different Monopoly Cards with equal probability. You need to collect them all to redeem the grand prize.

\begin{hw}
	Let $X$ be the number of visits you have to make before you can redeem the grand prize. Show that $\Var(X) = n^2\left(\sum{i=1}{n} i^{-2}\right) - \E[X]$.
\end{hw}
\begin{solution}
	To begin with, let us define the random variable $X = X_{1} + X_{2} + \ldots + X_{n}$, where $X_{i}$ represents the number of days until the $i^{th}$ coupon is obtained after $i-1$ coupons. Note here that $X_{i}$ is a geometric random variable.
	
	Now, we observe that $p_{i} = \frac{n-(i-1)}{n} = \frac{n-i+1}{n}$.
	
	With this, we see then that $\E[X_{i}] = \frac{1}{p_{i}} = \frac{n}{n-i+1}$.
	
	From here, we observe the following:
	\begin{align*}
		\E[X] &= \E[X_{1}] + \E[X_{2}] + \ldots + \E[X_{n-1}] + \E[X_{n}] \\
		&= \dfrac{n}{n} + \dfrac{n}{n-1} + \ldots + \dfrac{n}{2} + \dfrac{n}{1} \\
		&= \dfrac{n}{1} + \dfrac{n}{2} + \ldots + \dfrac{n}{n-1} + \dfrac{n}{n} \\
		&= n\left( \dfrac{1}{1} + \dfrac{1}{2} + \ldots + \dfrac{1}{n-1} + \dfrac{1}{n}\right) \\
		&= n \sum{i=1}{n} \dfrac{1}{i}
	\end{align*}

	Now from here, we want to calculate $\Var(X)$. Now, note here that each $X_{i}$ is independent; thus, we get that $\Var(X) = \Var(X_{1}) + \Var(X_{2}) + \ldots + \Var(X_{n-1}) + \Var(X_{n}) = \sum{i=1}{n} \Var(X_{i})$.
	
	From here, we observe that because $X_{i}$ is a geometric random variable, then $\Var(X_{i}) = \frac{1-p_{i}}{p_{i}^{2}}$. With this in mind, we see that:
	\begin{align*}
		\Var(X) &= \sum{i=1}{n} \Var(X_{i}) \\
		&= \sum{i=1}{n} \dfrac{1-p_{i}}{p_{i}^{2}} \\
		&= \sum{i=1}{n} \dfrac{1-\dfrac{n-i+1}{n}}{\left( \dfrac{n-i+1}{n} \right)^{2}} \\
		&= \sum{i=1}{n} \dfrac{n(i-1)}{(n-i+1)^{2}} \\
		&= n\sum{i=1}{n} \dfrac{i-1}{(n-i+1)^{2}}
	\end{align*}

	Now, we observe that $\sum{i=1}{n} \dfrac{i-1}{(n-i+1)^{2}}$ can be written as the following:
	\begin{align*}
		\sum{i=1}{n} \dfrac{i-1}{(n-i+1)^{2}} &= \dfrac{0}{n^{2}} + \dfrac{1}{(n-1)^{2}} + \ldots + \dfrac{n-2}{2^{2}} + \dfrac{n-1}{1^{2}} \\
		&= \dfrac{n-1}{1^{2}} +  \dfrac{n-2}{2^{2}} + \ldots + \dfrac{1}{(n-1)^{2}} + \dfrac{0}{n^{2}} \\
		&= \sum{i=1}{n} \dfrac{n-i}{i^{2}} \\
		&= \sum{i=1}{n} \dfrac{n}{i^{2}} - \sum{i=1}{n} \dfrac{1}{i} \\
		&= n\sum{i=1}{n} \dfrac{1}{i^{2}} - \sum{i=1}{n} \dfrac{1}{i}
	\end{align*}

	Thus, from this, we get the following:
	\begin{align*}
		n\sum{i=1}{n} \dfrac{i-1}{(n-i+1)^{2}} &= n\left( n\sum{i=1}{n} \dfrac{1}{i^{2}} - \sum{i=1}{n} \dfrac{1}{i} \right) \\
		&= n^{2}\sum{i=1}{n} \dfrac{1}{i^{2}} - n\sum{i=1}{n} \dfrac{1}{i} \\
		&= n^{2}\sum{i=1}{n} i^{-2} - \E[X]
	\end{align*}
\end{solution}

\newpage

\section{Probabilistically Buying Probability Books}
Chuck will go shopping for probability books for $K$ hours. Here, $K$
is a random variable and is equally likely to be 1, 2, or 3. The
number of books $N$ that he buys is random and depends on how long he shops. We are told that
\[
\Pr[N = n \mid K = k] =
\begin{cases}
	\frac{c}{k} & \text{for}\ n=1,\ldots,k \\
	0 & \text{otherwise}
\end{cases}
\]
for some constant $c$.

\begin{hw}
	Compute $c$.
\end{hw}
\begin{solution}
	We observe that $\Pr[N = n \mid K = k] = \frac{c}{k}$ for $n = 1, \ldots, k$. Since $n$ has $k$ values it can take on, and we know that $\sum{n=1}{k} \Pr[N=n \mid K = k] = 1$, we have then that $\frac{kc}{k} = 1$, meaning that $c = 1$.
\end{solution}

\begin{hw}
	Find the joint distribution of $K$ and $N$.
\end{hw}
\begin{solution}
	We observe that $\Pr[K] = \frac{1}{3}$, and $\Pr[N] = \frac{1}{k}$. Thus, we have the following:
	\[
	\Pr[N=n, K=k] =
	\begin{cases}
		\frac{1}{3} \cdot \frac{1}{k} = \frac{1}{3k} & \text{for}\ k = 1,2,3 \ \text{and}\ n = 1, \ldots, k \\
		0 & \text{otherwise}
	\end{cases}
	\]
\end{solution}

\begin{hw}
	Find the marginal distribution of $N$.
\end{hw}
\begin{solution}
	First, observe that $\Pr[N=n] = \sum{k=n}{3} \Pr[N=n, K=k] = \sum{k=n}{3} \frac{1}{3k}$. Then, from here, we get the following:
	\[
	\Pr[N=n] =
	\begin{cases}
		\frac{1}{3} + \frac{1}{6} + \frac{1}{9} = \frac{11}{18} & \text{for}\ n=1 \\
		\frac{1}{6} + \frac{1}{9} = \frac{5}{18} & \text{for}  \ n=2 \\
		\frac{1}{9} = \frac{2}{18} & \text{for}\ n=3 \\
		0 & \text{otherwise}
	\end{cases}
	\]
\end{solution}

\begin{hw}
	Find the conditional distribution of $K$ given that $N=1$.
\end{hw}
\begin{solution}
	We now want to find $\Pr[K = k \mid N = 1]$. To do this, recall the following:
	\begin{equation*}
		\Pr[K = k \mid N = 1] = \dfrac{\Pr[K=k, N = 1]}{\Pr[N=1]}
	\end{equation*}

	From the previous part, we know that $\Pr[N=1] = \frac{11}{18}$. Then, with this in mind, we get the following:
	\[
	\Pr[K = k \mid N = 1] =
	\begin{cases}
		\frac{\frac{1}{3}}{\frac{11}{18}} = \frac{6}{11} & k = 1 \\
		\frac{\frac{1}{6}}{\frac{11}{18}} = \frac{3}{11} & k = 2 \\
		\frac{\frac{1}{9}}{\frac{11}{18}} = \frac{2}{11} & k = 3 \\
		0 & \text{otherwise}
	\end{cases}
	\]
\end{solution}

\begin{hw}
	We are now told that he bought at least 1 but no more than 2 books. Find the conditional mean and variance of $K$, given this piece of information.
\end{hw}
\begin{solution}
	To start, let $A$ denote the event in which $1 \leq N \leq 2$. Observe that $\Pr[A] = \Pr[N = 1] + \Pr[N = 2] = \frac{8}{9}$.
	
	Now, observe that $\Pr[K = k \mid A] = \frac{\Pr[K = k, A]}{\Pr[A]}$.
	
	Next, observe that:
	\[
	\Pr[K=k, A] =
	\begin{cases}
		\frac{1}{3} & k = 1 \\
		\frac{1}{6} + \frac{1}{6} = \frac{1}{3} & k = 2 \\
		\frac{1}{9} + \frac{1}{9} = \frac{2}{9} & k = 3 \\
		0 & \text{otherwise}
	\end{cases}
	\]
	
	With this, we then see that
	\[
	\Pr[K = k \mid A] =
	\begin{cases}
		\frac{3}{8} & k = 1 \\
		\frac{3}{8} & k = 2 \\
		\frac{2}{8} & k = 3 \\
		0 & \text{otherwise}
	\end{cases}
	\]
	
	With this, we will find get:
	\begin{align*}
		\E[K \mid A] &= \left( 1 \right)\left( \dfrac{3}{8} \right) + \left( 2 \right)\left( \dfrac{3}{8} \right) + \left( 3 \right)\left( \frac{2}{8} \right) \\
		&= \dfrac{3}{8} + \dfrac{6}{8} + \dfrac{6}{8} \\
		&= \dfrac{15}{8}
	\end{align*}

	Now, to find the variance, we recall that $\Var(K \mid A) = \E[K^{2} \mid A] - \E[X \mid A]^{2}$. Therefore, we have
	\begin{align*}
		\Var(K \mid A) &= \E[K^{2} \mid A] - \E[X \mid A]^{2} \\
		&= \left( 1 \right)^{2}\left( \dfrac{3}{8} \right) + \left( 2 \right)^{2}\left( \dfrac{3}{8} \right) + \left( 3 \right)^{2}\left( \frac{2}{8} \right) - \left( \dfrac{15}{8} \right)^{2} \\
		&= \dfrac{39}{64}
	\end{align*}
\end{solution}

\begin{hw}
	The cost of each book is a random variable with mean 3.  What is the expectation of his total expenditure?
\end{hw}
\begin{solution}
	Let $B$ represent the total expenditure. We want to find $\E[B]$.
	
	Now, note that $\E[B] = \sum{i=1}{3} \E[B \mid N = i]\Pr[N = i]$. With this in mind, we have the following:
	\begin{align*}
		\E[B] &= \sum{i=1}{3} \E[B \mid N = i]\Pr[N = i] \\
		&= \E[B \mid N = 1]\Pr[N = 1] + \E[B \mid N = 2]\Pr[N = 2] + \E[B \mid N = 3]\Pr[N = 3] \\
		&= 3\left( \dfrac{11}{18} \right) + 6\left( \dfrac{5}{18} \right) + 9\left( \dfrac{2}{18} \right) \\
		&= \dfrac{9}{2}
	\end{align*}
\end{solution}

\newpage

\section{Dice Games}
\begin{hw}
	Alice and Bob are playing a game. Alice picks a random integer $X$ between 0 and 100 inclusive, where each value is equally likely to be chosen. Bob then picks a random integer $Y$ between $0$ and $X$ inclusive. What is $\E[Y]$?
\end{hw}
\begin{solution}
	To begin with, we will find $\E[Y \mid X = x]$. In this case, we observe that this is equal to $\frac{0 + x}{2} = \frac{x}{2}$. Note here that the probability of Alice picking some random integer is $\Pr[X = x] = \frac{1}{101}$.
	
	Now, using the Law of Total Expectation, we get the following:
	\begin{align*}
		\E[Y] &= \sum{x=0}{100} \E[Y \mid X = x]\Pr[X = x] \\
		&= \sum{x=0}{100} \left( \dfrac{x}{2} \right)\left( \dfrac{1}{101} \right) \\
		&= \dfrac{1}{202} \sum{x=0}{100} x \\
		&= \dfrac{1}{202}\left( \dfrac{101(0+100)}{2} \right) \\
		&= 25
	\end{align*}
\end{solution}

\begin{hw}
	Alice rolls a die until she gets a 1. Let $X$ be the number of total rolls she makes (including the last one), and let $Y$ be the number of rolls on which she gets an even number. Compute $\E[Y \mid X = x]$, and use it to calculate $\E[Y]$. 
\end{hw}
\begin{solution}
	We observe that $\E[Y \mid X = x] = \frac{3(x-1)}{5}$. Now, we observe that $\Pr[X = n] = \left( \frac{5}{6} \right)^{x-1}\left( \frac{1}{6} \right)$.
	
	From here, we see that:
	\begin{align*}
		\E[Y] &= \sum{n=1}{\infty} \E[Y \mid X = n] \Pr[X = n] \\
		&= \sum{n=1}{\infty} \left( \dfrac{3(x-1)}{5} \right) \left( \frac{5}{6} \right)^{x-1}\left( \frac{1}{6} \right)
	\end{align*}

	Now, by applying LOTUS for $f(X) = \frac{3(X-1)}{5}$, we have the following:
	\begin{align*}
		\sum{n=1}{\infty} \left( \dfrac{3(x-1)}{5} \right) \left( \frac{5}{6} \right)^{x-1}\left( \frac{1}{6} \right) &= \E\left[ \dfrac{3(X-1)}{5} \right] \\
		&= \dfrac{3}{5}\E[X-1] \\
		&= \dfrac{3}{5}\left( \E[X] - 1 \right) \\
		&= \dfrac{3}{5}\left( 6 - 1 \right) \\
		&= \dfrac{3}{5}(5) \\
		&= 3
	\end{align*}
\end{solution}

\begin{hw}
	Bob plays a game in which he starts off with one die. At each time step, he rolls all the dice he has. Then, for each die, if it comes up as an odd number, he puts that die back, and adds a number of dice equal to the number displayed to his collection. (For example, if he rolls a one on the first time step, he puts that die back along with an extra die.) However, if it comes up as an even number, he removes that die from his collection.
	
	What is the expected number of dice Bob will have after $n$ time steps?
\end{hw}
\begin{solution}
	We shall proceed by induction on $n$.
	
	First, let $X_{n}$ represent the total number of die Bob has after time step $n$. Observe that for $n = 0$, we have $\E[X_{0}] = 1$. Now, we observe that for $n=1$ time step, we have:
	\begin{align*}
		\E[X_{1}] &= \dfrac{1}{6}(1+1) + \dfrac{1}{6}(1+3) + \dfrac{1}{6}(1+5) \\
		&= 2
	\end{align*}

	Now, suppose that for time step $n = k \geq 1$, we have that $\E[X_{k}] = 2^{k}$.
	
	We now want to show that for time step $n=k+1$, $\E[X_{k+1}] = 2^{k+1}$. To do this, we observe that on time step $k$, we have $2^{k}$ dice. Now, for time step $k+1$, we observe that we can treat this as having $2^{k}$ dice each performing $1$ time step. This means that we have $2(2^{k}) = 2^{k+1}$ dice, as desired. Thus, we observe that for time step $k+1$, we have that $\E[X_{k+1}] = 2^{k+1}$.
	
	Therefore, we can conclude that the expected number of dice Bob will have after $n$ time steps is $2^{n}$.
\end{solution}
\end{document}